{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud detection: problem, solutions and tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The nature of the problem\n",
    "\n",
    "\"[Fraud is a billion-dollar business \n",
    "and it is increasing every year](https://en.wikipedia.org/wiki/Data_analysis_techniques_for_fraud_detection)\"\n",
    "\n",
    "What's a fraud? There are many formal definitions but essentially a fraud is an \"art\" and crime of deceiving and scamming people in their financial transactions. Frauds have always existed throughout human history but in this age of digital technology the strategy, extent and magnitude of financial frauds is becoming wide ranging - from credit cards transactions to health benefits to insurance claims. Fraudsters are also getting super creative in this digital age. Who's never received an email from a Nigerian royal family widow that she's looking for trusted someone to hand over large sums of her inheritance?\n",
    "No wonder why is fraud a big deal. It has been estimated that loses in business organizations can soar upto [4–5% of their revenues](https://wallethub.com/edu/cc/credit-debit-card-fraud-statistics/25725/) due to fraudulent transactions. A 5% fraud reduction may not sound a lot, but in monetary terms it is non-trivial and outweigh costs of not doing it by a large margin. A [PwC survey](https://www.pwc.com/gx/en/forensics/global-economic-crime-and-fraud-survey-2018.pdf) found that 50% of 7,200 companies they surveyed had been victims of fraud of some kind. A very recent [study by FICO](https://www.fico.com/blogs/real-time-payments-fraud?utm_source=social&utm_medium=social_platforms&utm_campaign=APAC_banks) have found that 4 out of 5 banks in their survey have experienced an increase in fraud activities and this is expected to rise in the future.  \n",
    "Although many organizations took measures to counter frauds it could never be eradicated. The goal is really to minimize its impacts and the benefits of this screening must be weighted against the costs, such as, investment in fraud detection technology and potentially loosing customers due to \"false positive\" alarms.  \n",
    "The purpose of this article is to highlight some tools, techniques and best practices in the field of fraud detection. Towards the end I'll provide a python implementation using publicly available dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Use cases\n",
    "Frauds are everywhere - where a transaction is involved-  but credit card fraud is probably the most known case. It can be as primitive as stealing or using stolen cards to aggressive forms such as account takeover, counterfeiting [and more](https://www.experian.com/blogs/ask-experian/credit-education/preventing-fraud/credit-card-fraud-what-to-do-if-you-are-a-victim/). Credit card frauds have always existed but the magnitude is only growing due to increasing online transactions taking place through credit cards everyday . According to a [Nilson Report](https://nilsonreport.com/upload/content_promo/The_Nilson_Report_10-17-2016.pdf) in 2010 the amount of global fraud was USD 7.6 billion, and is expected to cross a whopping USD 31 billion in 2020. In UK alone fraudulent transactions loses were estimated at more than [USD 1 billion in 2018](https://en.wikipedia.org/wiki/Credit_card_fraud).  \n",
    "The other kinds of big fraud cases are ongoing in insurance industries. Some estimates suggest that as much as 10% of health insurance claims in the US can be attributed to fraud - which is a non-trivial amount of USD 110 billion annually.  \n",
    "\n",
    "**Insurance fraud is so widespread that there is an entire organization called [Coalition Against Insurance Fraud](http://www.insurancefraud.org/) and a scientific journal called [Journal of Insurance Fraud](https://www.insurancefraud.org/jifa/jun-2016) to scientifically study frauds in the insurance business.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data science solution\n",
    "In the past (i.e. before machine learning became the trend) the standard practice was to use the so called \"Rule-based approach\". But every rule has an exception, so this technique was able to only partially mitigate the problem.\n",
    "With ever increasing online transactions and production of large volume of customer data, machine learning has been increasingly seen as an effective tool to detect and counter frauds. However, there is not a specific tool, the silver bullet, that work for all kinds of fraud detection problems in every industry. The nature of this problem is different in every case and every industry. Therefore every solution is carefully tailored within the domain of the industry and methods depend on the data and transaction types among others in each industry.  \n",
    "In machine learning parlance fraud detection is generally treated as a supervised classification problem, where observations are classified as \"fraud\" or \"non-fraud\" based on the features in those observations. It is also an interesting problem in ML research due to imbalanced data - i.e. very few cases of frauds in an extremely large amount of transactions. How to deal with imbalance classes is itself a subject of another discussion.  \n",
    "Frauds are also be isolated using outlier detection techniques. Outlier detection tools have their own way of tackling the problem, such as [time series analysis](https://www.datasciencecentral.com/profiles/blogs/outlier-detection-with-time-series-data-mining), cluster analysis, real-time monitoring of transactions etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Techniques\n",
    "**Statistical techniques:** average, quantiles, probability distribution, association rules  \n",
    "**Supervised ML algorithms:** classification, logistic regression, neural net, time-series analysis  \n",
    "**Unsupervised ML algorithms:** Cluster analysis, Bayesian network, Peer group analysis, break point analysis, Benford's law ( law of anomalous numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A simple Python implementation\n",
    "For this simple demo I am using a popular [Kaggle dataset](https://www.kaggle.com/mlg-ulb/creditcardfraud)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import data\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\DataS\\\\Google Drive\\\\DataScience\\\\Datasets\\\\creditcard.csv\")\n",
    "\n",
    "# view the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dataset has 31 columns. The first column \"Time\" is transaction time stamp, second last column \"Amount\" is transaction amount and the last column \"Class\" designates whether transaction as fraud or non-fraud (fraud = 1, non-fraud = 0).\n",
    "The rest of the columns, \"V1\" to \"V28\" are unknown variables which were transformed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frauds 492\n",
      "Non-frauds 284315\n"
     ]
    }
   ],
   "source": [
    "# number of fraud and non-fraud observations in the dataset\n",
    "frauds = len(df[df.Class == 1])\n",
    "nonfrauds = len(df[df.Class == 0])\n",
    "\n",
    "print(\"Frauds\", frauds); print(\"Non-frauds\", nonfrauds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scaling the \"Amount\" and \"Time\" columns similar to the others variables\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "rob_scaler = RobustScaler()\n",
    "\n",
    "df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "\n",
    "# now drop the original columns\n",
    "df.drop(['Time','Amount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y variables\n",
    "X = df.loc[:, df.columns != 'Class']\n",
    "y = df.loc[:, df.columns == 'Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Taking subsamples\n",
    " **This is an extremely unbalanced dataset so we need to take a subsample by undersampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of fraud cases\n",
    "frauds = len(df[df.Class == 1])\n",
    "\n",
    "# selecting the indices of the non-fraud classes\n",
    "fraud_indices = df[df.Class == 1].index\n",
    "nonfraud_indices = df[df.Class == 0].index\n",
    "\n",
    "# From all non-fraud observations, randomly select observations equal to number of fraud observations\n",
    "random_nonfraud_indices = np.random.choice(nonfraud_indices, frauds, replace = False)\n",
    "random_nonfraud_indices = np.array(random_nonfraud_indices)\n",
    "\n",
    "# Appending the 2 indices\n",
    "under_sample_indices = np.concatenate([fraud_indices,random_nonfraud_indices])\n",
    "\n",
    "# Under sample dataset\n",
    "under_sample_data = df.iloc[under_sample_indices,:]\n",
    "\n",
    "# Now split X, y variables from the under sample data\n",
    "X_undersample = under_sample_data.loc[:, under_sample_data.columns != 'Class']\n",
    "y_undersample = under_sample_data.loc[:, under_sample_data.columns == 'Class']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DataS\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\DataS\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "## split data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # The complete dataset\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)\n",
    "\n",
    "# Split dataset\n",
    "X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample\n",
    "                                                                                                   ,y_undersample\n",
    "                                                                                                   ,test_size = 0.3\n",
    "                                                                                                   ,random_state = 0)\n",
    "## modeling with logistic regression\n",
    "\n",
    "#import model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# instantiate model\n",
    "model = LogisticRegression()\n",
    "# fit \n",
    "model.fit(X_train_undersample, y_train_undersample)\n",
    "# predict\n",
    "y_pred = model.predict(X_test_undersample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Model evaluation\n",
    "**Note: Do not use accuracy score as a metric. In a dataset with 99.9% non-fraud observations, you will likely make correct prediction 99% of time. Confusion matrix and precision/recall score are better metric.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94       149\n",
      "           1       0.94      0.93      0.94       147\n",
      "\n",
      "    accuracy                           0.94       296\n",
      "   macro avg       0.94      0.94      0.94       296\n",
      "weighted avg       0.94      0.94      0.94       296\n",
      "\n",
      "CONFUSION MATRIX\n",
      "[[140   9]\n",
      " [ 10 137]]\n"
     ]
    }
   ],
   "source": [
    "# import classification report and confusion matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "classification_report = classification_report(y_test_undersample, y_pred)\n",
    "confusion_matrix = confusion_matrix(y_test_undersample, y_pred)\n",
    "\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(classification_report)\n",
    "print(\"CONFUSION MATRIX\") \n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End note:\n",
    "Thanks for reading this through. A Jupyter notebook along with the python demo can be found in my GitHub repo.   \n",
    "I can be reached via [Twitter](https://twitter.com/DataEnthus) or [LinkedIn](https://www.linkedin.com/in/mab-alam/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
